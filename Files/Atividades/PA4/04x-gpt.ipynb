{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAV2aLkfzPqf"
   },
   "source": [
    "# Arquitetura GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kjX_yOCzTlk"
   },
   "source": [
    "## 1 Distribuição de parâmetros do modelo GPT\n",
    "Na última aula você aprendeu como construir a estrutura do transformer para criar o modelo GPT por completo. Um transformer é basicamente composto por um módulo de atenção e um módulo feed forward. Já o modelo GPT é composto pela camada de embeddings, seguido de uma pilha de transformers, com uma camada linear de output no final. À medida que vamos aumentando as dimensões das matrizes e empilhando um número maior de transformers, a capacidade do modelo aumenta, mas a quantidade de parâmetros cresce consideravelmente, chegando na cada dos bilhões, ou até mesmo trilhões, de parâmetros. Daí vem o nome que conhecemos: Large Languange Models.\n",
    "\n",
    "Vamos verificar como estes pesos se distribuem ao longo da estrutura do modelo GPT para termos uma ideia da quantidade de pesos usada nos diferentes componentes do modelo. Para isso, recrie o modelo GPT visto na última aula, calcule e compare o número de parâmetros contidos na camada de embedding, nas camadas feed forward dos transformers, nos módulos de atenção dos transformers e a quantidade de parâmetros na camada de output geracional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYBeLrYxylxk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# DEFINA AS CLASSES PARA RECRIAR O MODELO GPTModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frRl1pPd7ayE"
   },
   "outputs": [],
   "source": [
    "# APENAS EXECUTE ESTA CÉLULA\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mceYX4r6EcCL"
   },
   "outputs": [],
   "source": [
    "emb_params_count = 0\n",
    "att_params_count = 0\n",
    "ff_params_count = 0\n",
    "output_params_count = 0\n",
    "\n",
    "\n",
    "# FAÇA A CONTAGEM DOS PARÂMETROS USANDO AS VARIÁVEIS DEFINIDAS ACIMA\n",
    "\n",
    "\n",
    "print(f\"Total de parâmetros da camada de embedding: {emb_params_count:,}\")\n",
    "print(f\"Total de parâmetros dos módulos de atenção: {att_params_count:,}\")\n",
    "print(f\"Total de parâmetros das camadas feed forward: {ff_params_count:,}\")\n",
    "print(f\"Total de parâmetros da camada de output geracional: {output_params_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-L3wGJpNIg1"
   },
   "source": [
    "## 2 Dropout separado\n",
    "\n",
    "Durante a criação do modelo GPT definimos apenas uma taxa de dropout, que é aplicada por todo o modelo. Altere o modelo para que existam 3 taxas de dropout distintas: uma na camada de embedding, outra na camada de shortcut e outra no módulo de atenção. Os parâmetros com as 3 taxas já foram definidos na configuração abaixo.\n",
    "\n",
    "Além de fazer a alteração para incluir os novos dropouts, imprima (dentro da classe mesmo) a porcentagem de valores da matriz que ficaram zerados logo após passar pelo dropout, isso servirá para debugarmos se o dropout está funcionando conforme o esperado.\n",
    "\n",
    "<small>DICA: conte os valores diferentes de zero, divida pelo total de elementos e subtraia 1 para contar a quantidade de zeros. <br>Ex: `(100 * (1 - torch.count_nonzero(x) / (x.numel())))`</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJ9eX2VZNy5Q"
   },
   "outputs": [],
   "source": [
    "# ALTERE AS CLASSES MultiHeadAttention, TransformerBlock, GPTModel\n",
    "# PARA ACRESCENTAR OS NOVOS DROPOUTS E IMPRIMIR A PORCENTAGEM DE PARÂMETROS\n",
    "# ZERADOS DAS MATRIZES APÓS PASSAR PELO DROPOUT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Djwm1UP8N2-r"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Não precisa alterar ou completar nada nesta célula, apenas execute e veja as\n",
    "porcentagens de elementos zerados de acordo com o print que você adicionou nas classes.\n",
    "'''\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate_emb\": 0.05,        # NOVO: dropout para camada de embedding\n",
    "    \"drop_rate_attn\": 0.10,       # NOVO: dropout para o módulo de attention\n",
    "    \"drop_rate_shortcut\": 0.15,   # NOVO: dropout para as shortcut connections\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[43, 15, 89],\n",
    "   [55, 87, 66],\n",
    "   [57, 85, 64],\n",
    "   [22, 58, 33],\n",
    "   [77, 25, 10],\n",
    "   [15, 80, 55]]\n",
    ")\n",
    "\n",
    "output = model(inputs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPUKR9180LlIaQxGjPQEVy0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
