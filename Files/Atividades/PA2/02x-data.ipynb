{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mn62ZgDN7qNd"
   },
   "source": [
    "# Preparação de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqBWP0GW2Jk4"
   },
   "source": [
    "## 1 Byte pair encoding de palavras fora do voculário\n",
    "\n",
    "Durante a aula vimos que um tokenizador baseado em Byte pair encoding (BPE) é capaz de lidar com palavras fora do vocabulário ao dividir uma palavra em \"sub-palavras\" que estejam presentes no vocabulário. Na pior das hipóteses a palavra pode ser quebrada em letras individuais.\n",
    "\n",
    "O texto abaixo é um trecho tirado do primeiro parágrafo do livro \"The Time Machine\" (H. G. Wells, 1895). Use o Tiktoken (com encoding do gpt2) visto durante a aula para tokenizá-lo e verifique quais palavras não estão presentes no vocabulário e necessitaram ser quebradas em \"sub-palavras\". Mostre como ficou a divisão de cada uma das palavras originalmente fora do vocabulário após a tokenização.\n",
    "\n",
    "Por exemplo, a palavra \"luxurious\":<br>\n",
    "`luxurious -> ['lux', 'urious']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.8.0\n",
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1750281188636,
     "user": {
      "displayName": "Rafael Glater",
      "userId": "13807686656890524906"
     },
     "user_tz": 180
    },
    "id": "1CXUaNqfuvSr"
   },
   "outputs": [],
   "source": [
    "time_machine_text = 'The Time Traveller was expounding a recondite matter to us. \\\n",
    "His grey eyes shone and twinkled, and his usually pale face was flushed and animated.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1750281190734,
     "user": {
      "displayName": "Rafael Glater",
      "userId": "13807686656890524906"
     },
     "user_tz": 180
    },
    "id": "6TTFQ_-op__c",
    "outputId": "fa382b7c-aabc-4452-f29d-ad29bd4b9df9"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# SEU CÓDIGO AQUI\n",
    "def Q1():\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    for word in time_machine_text.split():\n",
    "        encoded_word = tokenizer.encode(word, allowed_special={\"<|endoftext|>\"})\n",
    "        decoded_segments = [tokenizer.decode([i]) for i in encoded_word]\n",
    "        print(f\"{word} -> {decoded_segments}\")\n",
    "\n",
    "Q1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIr_d1CI8H8v"
   },
   "source": [
    "## 2 Data loader com diferentes tamanhos de contexto e strides\n",
    "\n",
    "Durante a aula, vimos como criar um data loader pra treinar uma LLM através da tarefa de prever o próximo token. No caso, o input `x` é uma sequência de tokens e o alvo `y` é o próximo token da sequência `x`. O data loader cria uma janela deslizante que percorre todo o texto, gerando inúmeros exemplos de treino `x, y`. A quantidade de dados de treino gerada pelo data loader vai variar de acordo com o tamanho de `x` (`max_length`) e o tanto que a janela irá deslizar (`stride`) ao longo do texto.\n",
    "\n",
    "Use o data loader visto durante a aula para tokenizar o texto abaixo com duas configurações distintas:\n",
    "- `batch_size=4, max_length=2, stride=1`\n",
    "- `batch_size=4, max_length=6, stride=2`\n",
    "\n",
    "E responda, quantos exemplos de treino cada configuração o data loader gerou? Lembre-se que cada batch pode conter até 4 exemplos de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1750283362465,
     "user": {
      "displayName": "Rafael Glater",
      "userId": "13807686656890524906"
     },
     "user_tz": 180
    },
    "id": "0cSwpsUVAyiY"
   },
   "outputs": [],
   "source": [
    "time_machine_text = \"The Time Traveller (for so it will be convenient to speak of him) \\\n",
    "was expounding a recondite matter to us. His grey eyes shone and \\\n",
    "twinkled, and his usually pale face was flushed and animated. The \\\n",
    "fire burned brightly, and the soft radiance of the incandescent \\\n",
    "lights in the lilies of silver caught the bubbles that flashed and \\\n",
    "passed in our glasses. Our chairs, being his patents, embraced and \\\n",
    "caressed us rather than submitted to be sat upon, and there was that \\\n",
    "luxurious after-dinner atmosphere when thought roams gracefully \\\n",
    "free of the trammels of precision. And he put it to us in this \\\n",
    "way--marking the points with a lean forefinger--as we sat and lazily \\\n",
    "admired his earnestness over this new paradox (as we thought it) \\\n",
    "and his fecundity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6339,
     "status": "ok",
     "timestamp": 1750283442438,
     "user": {
      "displayName": "Rafael Glater",
      "userId": "13807686656890524906"
     },
     "user_tz": 180
    },
    "id": "xQm82iBRBMUM"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# SEU CÓDIGO COM A CLASSE DO DATASET E A FUNÇÃO DO DATA LOADER\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1750283623598,
     "user": {
      "displayName": "Rafael Glater",
      "userId": "13807686656890524906"
     },
     "user_tz": 180
    },
    "id": "v8Qj6jRVBccZ",
    "outputId": "db0daea2-b60e-44a5-b57d-175534084c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para a configuração: {'batch_size': 4, 'max_length': 2, 'stride': 1} o data loader 168 exemplos de treino.\n",
      "(tensor([ 464, 3862]), tensor([ 3862, 43662]))\n",
      "{'tokenized': 170, 'dataset': 168, 'train_examples_count': 168, 'words': 128, 'batches': 42, 'tests': 2}\n"
     ]
    }
   ],
   "source": [
    "# CHAME O DATA LOADER COM TEXTO ACIMA COM A 1ª CONFIGURAÇÃO\n",
    "# E CONTE OS EXEMPLOS DE TREINO\n",
    "\n",
    "configs = {\n",
    "    1: {\"batch_size\": 4, \"max_length\": 2, \"stride\": 1},\n",
    "    2: {\"batch_size\": 4, \"max_length\": 6, \"stride\": 2},\n",
    "}\n",
    "\n",
    "\n",
    "def count_text_examples(text, config):\n",
    "    dataloader = create_dataloader_v1(\n",
    "        txt=text,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        stride=config[\"stride\"],\n",
    "    )\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    encoded_text = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "    counts = {\n",
    "        \"tokenized\": len(encoded_text),\n",
    "        # \"calculated\": (len(encoded_text) - config['max_length']) // config['stride'] + 1,\n",
    "        \"dataset\": len(dataloader.dataset),\n",
    "        \"train_examples_count\": len(dataloader)*config['batch_size'],\n",
    "        \"words\": len(text.split()),\n",
    "        \"batches\": len(dataloader),\n",
    "        \"tests\": len(dataloader.dataset[0]),\n",
    "    }\n",
    "    print(f\"Para a configuração: {config} o data loader {counts['train_examples_count']} exemplos de treino.\")\n",
    "    for item in dataloader.dataset:\n",
    "        print(item)\n",
    "        break\n",
    "    print(counts)\n",
    "\n",
    "count_text_examples(time_machine_text, configs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1750283651730,
     "user": {
      "displayName": "Rafael Glater",
      "userId": "13807686656890524906"
     },
     "user_tz": 180
    },
    "id": "i6j0G9dcCOK4",
    "outputId": "a07bb269-12ce-47e9-dd22-4db88737972a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para a configuração: {'batch_size': 4, 'max_length': 6, 'stride': 2} o data loader 80 exemplos de treino.\n",
      "(tensor([  464,  3862, 43662,  6051,   357,  1640]), tensor([ 3862, 43662,  6051,   357,  1640,   523]))\n",
      "{'tokenized': 170, 'dataset': 82, 'train_examples_count': 80, 'words': 128, 'batches': 20, 'tests': 2}\n"
     ]
    }
   ],
   "source": [
    "# CHAME O DATA LOADER COM TEXTO ACIMA COM A 2ª CONFIGURAÇÃO\n",
    "# E CONTE OS EXEMPLOS DE TREINO\n",
    "\n",
    "count_text_examples(time_machine_text, configs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39mTraceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader\u001b[49m.dataset:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(item)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP2pZUkQiOJdPzwIUzIXjs7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
